{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e062f70-76a8-404a-994d-c1d134d62c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "    @staticmethod\n",
    "    def loss(theta, x, y, lambda_param=None):\n",
    "        \"\"\"Loss function for logistic regression with without regularization\"\"\"\n",
    "        exponent = - y * (x.dot(theta))\n",
    "        return np.sum(np.log(1+np.exp(exponent))) / x.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(theta, x, y, lambda_param=None):\n",
    "        \"\"\"\n",
    "        Gradient function for logistic regression without regularization.\n",
    "        Based on the above logistic_regression\n",
    "        \"\"\"\n",
    "        exponent = y * (x.dot(theta))\n",
    "        gradient_loss = - (np.transpose(x) @ (y / (1+np.exp(exponent)))) / (\n",
    "            x.shape[0])\n",
    "\n",
    "        # Reshape to handle case where x is csr_matrix\n",
    "        gradient_loss.reshape(theta.shape)\n",
    "\n",
    "        return gradient_loss\n",
    "\n",
    "\n",
    "class LogisticRegressionSinglePoint():\n",
    "    @staticmethod\n",
    "    def loss(theta, xi, yi, lambda_param=None):\n",
    "        exponent = - yi * (xi.dot(theta))\n",
    "        return np.log(1 + np.exp(exponent))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(theta, xi, yi, lambda_param=None):\n",
    "\n",
    "        # Based on page 22 of\n",
    "        # http://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect09.pdf\n",
    "        exponent = yi * (xi.dot(theta))\n",
    "        return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "\n",
    "class LogisticRegressionRegular():\n",
    "    @staticmethod\n",
    "    def loss(theta, x, y, lambda_param):\n",
    "        regularization = (lambda_param/2) * np.sum(theta*theta)\n",
    "        return LogisticRegression.loss(theta, x, y) + regularization\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(theta, x, y, lambda_param):\n",
    "        regularization = lambda_param * theta\n",
    "        return LogisticRegression.gradient(theta, x, y) + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1a8281-fb3e-4423-b668-589c260bc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions from in-class exercises\n",
    "'''\n",
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def laplace_mech_vec(vec, sensitivity, epsilon):\n",
    "    return [v + np.random.laplace(loc=0, scale=sensitivity / epsilon) for v in vec]\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a181eb98-d44b-45f1-ac6f-657e6ef8b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "url_x = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_x.npy'\n",
    "url_y = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_y.npy'\n",
    "\n",
    "with urllib.request.urlopen(url_x) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "X = np.load(f)\n",
    "\n",
    "with urllib.request.urlopen(url_y) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2667d8-4558-4314-9b1d-8663c716e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set sizes: 36176 9044\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3990dfbf-1284-4bbb-b0d2-82ba2f150f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions taken from in-class-exercise 10.28.24\n",
    "'''\n",
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias) #this is the dot product and take the sign. \n",
    "    return label\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2) #computing L2 norm \n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "        \n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)\n",
    "#theta = [-.1 for _ in range(104)]\n",
    "#accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701da67-a821-4b8a-a213-b95f27d92d43",
   "metadata": {},
   "source": [
    "### IMPLEMENTING BATCH GRADIENT DESCENT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fbd54f8d-21d9-4c79-8899-3921adca2b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy with epochs = 20, learning rate = .1:   0.7585139318885449\n",
      "Final accuracy with epochs = 40, learning rate = .1:   0.7587350729765591\n",
      "Final accuracy with epochs = 60, learning rate = .1:   0.7628261831048209\n",
      "Final accuracy with epochs = 80, learning rate = .1:   0.7778637770897833\n",
      "Final accuracy with epochs = 100, learning rate = .1:   0.7807386112339673\n",
      "Final accuracy with epochs = 20, learning rate = 1:   0.8081601061477223\n",
      "Final accuracy with epochs = 40, learning rate = 1:   0.8159000442282176\n",
      "Final accuracy with epochs = 60, learning rate = 1:   0.8182220256523662\n",
      "Final accuracy with epochs = 80, learning rate = 1:   0.8209862892525431\n",
      "Final accuracy with epochs = 100, learning rate = 1:   0.8228659885006634\n"
     ]
    }
   ],
   "source": [
    "def acuracy(theta):\n",
    "    predictions = np.sign(X_train.dot(theta))  # Apply the sign function to get class predictions\n",
    "    return np.mean(predictions == y_train)\n",
    "\n",
    "def batch_gradient_descent(epochs, n):\n",
    "    \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    regression = LogisticRegression()\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for _ in range(epochs):\n",
    "        # Compute gradient using the vectorized function\n",
    "        \n",
    "        grad = regression.gradient(theta, X_train, y_train, lambda_param=None) \n",
    "\n",
    "        # Update the parameters (theta)\n",
    "        theta -= n * grad\n",
    "\n",
    "    #print(theta)\n",
    "    return theta\n",
    "\n",
    "theta0 = batch_gradient_descent(20, .1)\n",
    "theta1 = batch_gradient_descent(40, .1)\n",
    "theta2 = batch_gradient_descent(60, .1)\n",
    "theta3 = batch_gradient_descent(80, .1)\n",
    "theta4 = batch_gradient_descent(100, .1)\n",
    "theta5 = batch_gradient_descent(20, 1)\n",
    "theta6 = batch_gradient_descent(40, 1)\n",
    "theta7 = batch_gradient_descent(60, 1)\n",
    "theta8 = batch_gradient_descent(80, 1)\n",
    "theta9 = batch_gradient_descent(100, 1)\n",
    "\n",
    "print('Final accuracy with epochs = 20, learning rate = .1:  ', accuracy(theta0))\n",
    "print('Final accuracy with epochs = 40, learning rate = .1:  ', accuracy(theta1))\n",
    "print('Final accuracy with epochs = 60, learning rate = .1:  ', accuracy(theta2))\n",
    "print('Final accuracy with epochs = 80, learning rate = .1:  ', accuracy(theta3))\n",
    "print('Final accuracy with epochs = 100, learning rate = .1:  ', accuracy(theta4))\n",
    "print('Final accuracy with epochs = 20, learning rate = 1:  ', accuracy(theta5))\n",
    "print('Final accuracy with epochs = 40, learning rate = 1:  ', accuracy(theta6))\n",
    "print('Final accuracy with epochs = 60, learning rate = 1:  ', accuracy(theta7))\n",
    "print('Final accuracy with epochs = 80, learning rate = 1:  ', accuracy(theta8))\n",
    "print('Final accuracy with epochs = 100, learning rate = 1:  ', accuracy(theta9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961932eb-1c3a-4792-908a-551e3589a180",
   "metadata": {},
   "source": [
    "### EPSILON DP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efed7bc4-2401-4711-acc1-6703e6dea3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.23815975   5.78745617   1.00852662  -1.75662619  -3.41198047\n",
      "  -4.80469834   6.64370471  -0.86000989   2.62906297   0.76557664\n",
      "   0.53598117   1.42502169   3.97435256   1.48369348   6.39285508\n",
      "  -0.62831194   6.79219427   5.02449443   0.96698887  -4.51679907\n",
      "  -0.58858787  -0.96942414  10.62507056  -0.46960108  -1.6105687\n",
      "  -3.09863144   4.13230502  -4.7383503    1.59448386  -2.1213927\n",
      "  -6.77310865   5.33738851   3.97783674  -8.80844523  -0.401746\n",
      "   1.71818944   3.43665327 -14.22137987   1.57265143  -0.308636\n",
      "  -1.97016339  -1.59255716  -0.22005702   8.39779865  -0.62351416\n",
      " -10.18477711  -9.02416568   2.04287195  -7.53609551   1.43248538\n",
      "   3.58755683   2.02823716   0.14902395   2.07261662  -6.21659812\n",
      "   6.02064628  10.06618258  -1.26192237   3.39608873   4.89034576\n",
      "   1.49755143  -0.19261116   2.78852041  -6.58004916 -14.31835326\n",
      "  -8.19017156   3.36573215   6.73976769  -6.16409321   1.28512722\n",
      "  -1.65828633   4.61174212   3.93692728  -7.62879656  -4.66214534\n",
      "  -2.26875856  10.4114509   -3.16210832  -5.05195787   2.7809985\n",
      "  -3.88718433  -1.39331494   1.88684702   4.59913781  -2.89450014\n",
      "  -1.74419421  12.60539794   4.79457175  -4.61420236  -8.84573228\n",
      "  -7.21529358  -4.97137921   9.30809856  -1.35849169   7.49098323\n",
      "  -4.68704558  -1.49340698   0.40797601   6.81984528  -8.28909988\n",
      "   1.70301337   1.22487949   3.05039999   0.89609274]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5699911543564794"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient_epsilon_descent(epochs, epsilon, n):\n",
    "    \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    regression = LogisticRegression()\n",
    "    epsilon_i = epsilon/epochs\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for _ in range(epochs):\n",
    "        # Compute gradient using the vectorized function\n",
    "        \n",
    "        grad = regression.gradient(theta, X_train, y_train, lambda_param=None)\n",
    "        grad_noisy = laplace_mech_vec(grad, sensitivity=1, epsilon=epsilon_i)\n",
    "        noisy_array = np.array(grad_noisy)\n",
    "\n",
    "        # Update the parameters (theta)\n",
    "        theta -= n * noisy_array\n",
    "\n",
    "    print(theta)\n",
    "    return theta\n",
    "\n",
    "theta = batch_gradient_epsilon_descent(50, 100, 1) #Needs a high epsilon\n",
    "theta\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14826b67-dc4b-4a0a-bfae-7dc5dfc81419",
   "metadata": {},
   "source": [
    "### EPSILON DELTA DP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "eac1e125-34d7-4dbe-be07-2fe623fa5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -49.41731588   30.37049136   11.45693737  -30.20274393  -88.07933288\n",
      "  -35.35845856   14.28665606  100.68227408  -23.140508    -47.3081694\n",
      "   -7.35290573   52.39077016    8.94129519   53.58991605   24.79771902\n",
      "  -97.65490888  102.07186543  -73.71000536  -32.28139141  -22.90670257\n",
      "   -6.51303884  -84.77268749   18.2648349    55.3874417   -30.96521827\n",
      "  105.19868052  -93.74657329    7.69245444  -39.24928539   21.24814705\n",
      "  -49.33031786  -23.62941913   21.40154993  -14.36268552   81.80375251\n",
      "  -24.17940414  -69.11407754  -30.24454097   69.75002965   -6.95658538\n",
      "  -35.5212063    59.76883197   58.23824783  117.32047512  -67.45789463\n",
      "  -16.68675865   42.89123458  -21.2459076    36.41038241  -94.41760507\n",
      " -129.15517725   81.5388054   -30.41120409  -38.96166682   29.41256427\n",
      "   50.82288111 -124.8568815    23.92797033   76.02150176    6.37276324\n",
      " -105.18611626  -92.50859826 -104.23732649   43.67335521 -112.38387683\n",
      "   66.36136194  -70.56529625  -41.47587296  -60.67434777 -100.68379199\n",
      "   68.81136525  -66.10308673   56.42791862   32.17370403  -10.90032861\n",
      "  -43.51726092  -57.89677136  -21.2024023    -3.90060708  -61.2982722\n",
      "  -22.64445178  199.3675155    43.49189808    6.32573583   42.05930051\n",
      "  -39.02458924   54.08775716   74.10764794  -99.71864357   -9.30726458\n",
      "   45.37836179  -53.8048314   -75.58754144   -8.72125316  -40.13577918\n",
      "   23.2399125    66.27954962   59.80710494  -38.7752971   -50.80688604\n",
      "  -53.59316566  -34.37708391  -59.47820918   48.36476897]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5222246793454224"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gradient_epsilon_delta_descent(epochs, epsilon, delta, n):\n",
    "    \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    regression = LogisticRegression()\n",
    "    epsilon_i = epsilon/epochs\n",
    "    delta_i = delta/epochs\n",
    "    b=3\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for _ in range(epochs):\n",
    "        # Compute gradient using the vectorized function\n",
    "        \n",
    "        grad = regression.gradient(theta, X_train, y_train, lambda_param=None)\n",
    "        clipped_grad = L2_clip(grad, b)\n",
    "        grad_noisy = gaussian_mech_vec(clipped_grad, sensitivity=b, epsilon=epsilon_i, delta = delta_i)\n",
    "        noisy_array = np.array(grad_noisy)\n",
    "\n",
    "        # Update the parameters (theta)\n",
    "        theta -= n * noisy_array\n",
    "\n",
    "    #print(theta)\n",
    "    return theta\n",
    "\n",
    "theta = batch_gradient_epsilon_delta_descent(50, 100, 1e-5, 1) #Needs a high epsilon\n",
    "theta\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f7012-8996-4acf-b126-5b41e6b06552",
   "metadata": {},
   "source": [
    "### RDP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8c00f99f-3676-4160-a017-0793dd74a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.54657349  5.02974815  2.35003439 -0.01703512  0.28942141 -2.61561595\n",
      " -5.26921933 -2.43695499  8.46912989  1.76271193 -4.65601623 -3.00029351\n",
      "  2.29102998  3.45498216 -0.75804558  1.25043994  1.12620038  1.4250339\n",
      " -5.12023657  3.17958699 -1.35816615  2.5008328  -0.80356306  3.96753397\n",
      " -7.64242691  0.63116457 -0.05379498 -2.46367853  6.81642045  0.68310843\n",
      "  4.0034507   5.86168102 -1.38098818  0.0205345   1.16443706 -6.20750007\n",
      " -1.96976079  0.9722609  -1.87762097 -0.60806927 -1.96562999  4.45232717\n",
      "  0.56502517 -0.64752587 -0.60460136 -1.37211564 -0.16804562 -3.11666514\n",
      " -4.0186661   1.91260285  2.75676419  9.94573596 -1.87167689  1.65947714\n",
      " -1.42124375 -2.34339042  3.70056675 -0.47323726  0.77364924 -2.4619316\n",
      "  2.82227807 -0.86610472 -7.85214669  3.70250752  3.34593545 -0.64976129\n",
      "  1.73212049  3.32917508  1.61334875 -2.4505212  -0.35716404  5.77131929\n",
      "  7.55902796  0.39753227  9.68056554 -2.23330871  3.04656239  1.12539396\n",
      "  0.80675537  2.42483223 -5.16090887 -6.46953632 -6.1071015   1.93256826\n",
      " -6.8153414  -2.1931747   7.87336777 10.93742835  3.73120648  0.71090566\n",
      "  1.62810408  3.73870203  1.39868644  3.8619505   4.90122516 -7.72728217\n",
      " -0.45647672  3.42531026  2.30822287 -1.04363328  4.01856403 -1.94497936\n",
      "  2.17061389 -3.91500421]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.706766917293233"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "def batch_gradient_rdp_descent(epochs, epsilon_bar, alpha, n):\n",
    "    \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    regression = LogisticRegression()\n",
    "    \n",
    "    epsilon_i = epsilon_bar/epochs\n",
    "    alpha_i = alpha/epochs\n",
    "    b=3\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for _ in range(epochs):\n",
    "        # Compute gradient using the vectorized function\n",
    "        \n",
    "        grad = regression.gradient(theta, X_train, y_train, lambda_param=None)\n",
    "        clipped_grad = L2_clip(grad, b)\n",
    "        grad_noisy = gaussian_mech_RDP_vec(clipped_grad, sensitivity=b, alpha=alpha_i, epsilon=epsilon_i)\n",
    "        noisy_array = np.array(grad_noisy)\n",
    "\n",
    "        # Update the parameters (theta)\n",
    "        theta -= n * noisy_array\n",
    "\n",
    "    print(theta)\n",
    "    return theta\n",
    "\n",
    "theta = batch_gradient_rdp_descent(50, 100, 10, 1) #Needs a high epsilon\n",
    "theta\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d065c05-c6a4-45c5-bb63-75236156c2a2",
   "metadata": {},
   "source": [
    "### zCDP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c6c92e09-4a68-4eaf-a223-0e17f8e418e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.46107744  18.71017285   7.28563567  -4.54468891   5.01906203\n",
      "   7.59812795   6.36195557   7.33400508  -6.95330201  -2.26451643\n",
      "   3.37230708   9.16754422  -9.52830718  -4.09790484   0.93176367\n",
      "  -8.97875391 -18.70332643   2.98311248   2.53154384  -4.33785959\n",
      "  10.09436114 -10.83627957   6.46440944  -0.39808524  -6.30948589\n",
      "  17.01794052   8.43970677  -6.60103456  -4.07385403   0.17903216\n",
      "  -5.15959474  -0.41420193  -2.35377986   9.09503676  -2.79623215\n",
      "   1.08663891   1.19205575  -7.285804   -14.6534904   -5.38105877\n",
      "  16.79749162   4.60229295   9.39628074  -1.82150649  -3.92576991\n",
      " -13.73777097 -13.31216002  -8.07285285  -3.93243221  15.51058605\n",
      "   7.16723023   6.84648798   6.21616575   1.46579858  -2.43688902\n",
      "  -7.37710042 -11.01506289   4.53785564   7.90284772 -12.62762025\n",
      "   6.74948835  19.5802878   -5.92765874   6.48700362 -13.39905439\n",
      "  16.34471642  -5.01908456  -5.02743027  10.07750738 -29.40813581\n",
      "   0.36457593   9.34901734   9.36591867   5.76699937  12.71497993\n",
      "  -4.80664749  14.54298244  -3.59650606  -6.76705964 -21.82133988\n",
      "   7.19318817   0.76717443 -10.30081216  -0.62364095  -9.76131261\n",
      "  -0.78139219  23.58543474  -3.72610113 -11.01444514  15.3779619\n",
      "  -8.06369588 -20.48493731   7.72197751  12.32209873  13.64365664\n",
      "  -8.04499611  -4.43266292  10.83086955  -9.96686484  -0.05616457\n",
      "  -8.31546297  -6.28561374  15.27285421   9.4090757 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7289915966386554"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "def batch_gradient_zcdp_descent(epochs, rho, n):\n",
    "    \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    regression = LogisticRegression()\n",
    "    \n",
    "    rho_i = rho/epochs\n",
    "    b=3\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for _ in range(epochs):\n",
    "        # Compute gradient using the vectorized function\n",
    "        \n",
    "        grad = regression.gradient(theta, X_train, y_train, lambda_param=None)\n",
    "        clipped_grad = L2_clip(grad, b)\n",
    "        grad_noisy = gaussian_mech_zCDP_vec(clipped_grad, sensitivity=b, rho=rho_i)\n",
    "        noisy_array = np.array(grad_noisy)\n",
    "\n",
    "        # Update the parameters (theta)\n",
    "        theta -= n * noisy_array\n",
    "\n",
    "    print(theta)\n",
    "    return theta\n",
    "\n",
    "theta = batch_gradient_zcdp_descent(50, 100, 1) #Needs a high epsilon\n",
    "theta\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "262b40be-d6d0-47a2-b2cf-614423bcce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving forward - need to convert to epsilon-delta for zCDP and RDP to determine \n",
    "# which produces the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124d9bb-2286-49ed-903c-7ead0905098c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
