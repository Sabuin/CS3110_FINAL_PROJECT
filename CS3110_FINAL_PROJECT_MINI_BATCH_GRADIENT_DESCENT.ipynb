{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392e364d-28ff-4e1d-8da4-ca347c44d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions from in-class exercises\n",
    "'''\n",
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def laplace_mech_vec(vec, sensitivity, epsilon):\n",
    "    return [v + np.random.laplace(loc=0, scale=sensitivity / epsilon) for v in vec]\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "\n",
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]\n",
    "    \n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26874e7-14f6-4ea8-94e9-b2ba6d1b6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "url_x = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_x.npy'\n",
    "url_y = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_y.npy'\n",
    "\n",
    "with urllib.request.urlopen(url_x) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "X = np.load(f)\n",
    "\n",
    "with urllib.request.urlopen(url_y) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e25dc4c-b09c-419f-beca-03690c62335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set sizes: 36176 9044\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df526623-9050-4d44-962e-74b0f17d67ba",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MINI-BATCH GRADIENT DESCENT (WITHOUT DP FOR NOW)\n",
    "#### Steps to follow to implement the mini-batch gradient descent (without dp):\n",
    "1. Define a function that splits data into mini-batches (subsets of the whole dataset)\n",
    "2. Define loss function that measures how good our model is.\n",
    "3. Define gradient function. The gradient is a vector that indicates the rate of change of the loss in each direction.\n",
    "4. Define avg_grad function that computes the average gradient over each mini-batch.\n",
    "5. Define gradient_descent function that computes gradient using mini-batches for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df060ccc-3e0e-40fd-ade2-930e0bd4a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_mini_batches(X, y, batch_size):\n",
    "    # shuffling the data before creating mini_batches to prevent the model \n",
    "    # from learning possible patterns + each batch might contain more \"diversified\"\n",
    "    # information. \n",
    "    \n",
    "    shuffled_data = np.random.permutation(X.shape[0])\n",
    "    randomized_X = X[shuffled_data]\n",
    "    randomized_Y = y[shuffled_data]\n",
    "\n",
    "    mini_batches = []\n",
    "    for i in range(0,X.shape[0],batch_size):\n",
    "        mini_batches.append((randomized_X[i:i+batch_size], randomized_Y[i:i+batch_size]))\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a53398d-107a-453a-850c-5443795d1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions taken from in-class-exercise 10.28.24\n",
    "'''\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "#Vectorized version of gradient calculation by wanglun1996. \n",
    "#Github: https://github.com/sunblaze-ucb/dpml-benchmark/blob/master/lossfunctions/logistic_regression.py#L12\n",
    "\n",
    "def gradient_vectorized(theta, x, y, lambda_param=None):\n",
    "    \"\"\"\n",
    "    Gradient function for logistic regression without regularization.\n",
    "    Based on the above logistic_regression\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    exponent = y * (x.dot(theta))\n",
    "    gradient_loss =  - (X.T @ (y / (1 + np.exp(exponent)))) / m\n",
    "\n",
    "    # Reshape to handle case where x is csr_matrix\n",
    "    gradient_loss.reshape(theta.shape)\n",
    "\n",
    "    return gradient_loss\n",
    "\n",
    "def avg_grad(theta, X, y):\n",
    "\n",
    "    #All_grads is a list of vectors, with each vector of length 104\n",
    "    all_grads = [gradient(theta,X[i],y[i]) for i in range(len(X))] #one gradient per example in the data\n",
    "\n",
    "    #Compute the column-wise average\n",
    "    avg_grad = np.mean(all_grads,axis=0)\n",
    "    \n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107e2ddb-aa61-4ae6-9913-b732566b0648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.18333295e-02, -7.24772591e-01, -6.18566005e-01, -3.94976018e-01,\n",
       "       -1.07397378e+00, -8.58378802e-01, -1.53725404e+00, -1.01144330e+00,\n",
       "       -9.48915075e-01, -7.35441122e-01, -1.41681288e+00, -8.69407314e-01,\n",
       "       -1.18777173e+00, -1.23182706e+00, -4.11682124e-02,  9.89539293e-02,\n",
       "        5.35657550e-01,  1.27113170e+00, -3.53622683e-01,  8.32468932e-01,\n",
       "       -1.47271301e+00,  1.34477683e+00,  1.00455231e-02, -1.57555629e+00,\n",
       "        1.39413902e+00,  8.48817751e-01, -1.17347103e+00, -1.92510572e+00,\n",
       "       -1.45620815e+00, -1.28870349e+00, -3.79294646e-01, -9.34284934e-02,\n",
       "       -1.35088113e-01,  4.76118543e-01, -1.22083233e+00, -9.53941577e-01,\n",
       "       -5.15858100e-01, -1.11066380e+00, -1.74509141e+00,  2.97435666e-01,\n",
       "        2.59958338e-01,  5.40176079e-03,  3.51595426e-01, -4.12399185e-01,\n",
       "       -9.18759091e-01, -5.17565340e-01, -1.39303274e+00, -1.59614174e+00,\n",
       "       -7.57287234e-01,  6.69822896e-03, -1.26099067e+00, -7.38852592e-01,\n",
       "       -1.17816769e+00, -1.11176309e+00, -8.86313872e-01, -3.03516565e+00,\n",
       "       -2.14092227e+00,  9.99961217e-01,  6.22204026e-01, -4.05386961e-01,\n",
       "       -1.70445634e+00,  1.43362113e-01, -7.98202577e-01, -4.37224501e-01,\n",
       "       -3.86128095e-01,  6.85272872e-01,  1.01862193e+00, -6.24594588e-02,\n",
       "        4.30277330e-02, -4.97115170e-02,  3.39469360e-01, -1.71181099e-02,\n",
       "        1.72717070e-02, -3.48487418e-01,  5.40851200e-01, -1.26827269e-01,\n",
       "        2.67218634e-01,  7.92663474e-01,  8.56210955e-01,  3.19294050e-01,\n",
       "       -1.90718681e-01, -8.41173595e-01, -5.65154133e-01, -5.47513118e-01,\n",
       "       -4.28694451e-01, -8.15270022e-01,  1.58694921e-01, -4.39880266e-02,\n",
       "        3.51767098e-01, -2.54690664e-01, -1.10868680e+00, -9.71372864e-01,\n",
       "       -2.31880418e-01, -7.52862958e-01, -6.24146087e-01,  2.57684249e-01,\n",
       "       -1.28776806e+00,  4.10258667e-01,  2.33527323e+00,  1.06530076e+00,\n",
       "        1.26380134e+00,  2.32216183e+01,  2.77134641e+00,  2.83597355e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Original function gradient_descent() taken from in-class-exercise 10.28.24 and modifying it\n",
    "iterate over the mini-batches instead of over the entire dataset (full-batch)\n",
    "'''\n",
    "def mini_batch_gradient_descent(epochs, batch_size):\n",
    "    #Step 1: initalize all thetas\n",
    "    theta = [0 for _ in range(X_train.shape[1])] #Initial model\n",
    "\n",
    "    #Step 2: split data into mini_batches\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "    #Step 3: iterate for each num samples in training set (training_set = mini batch)\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "            theta = theta - avg_grad(theta, X_train_batch, y_train_batch)\n",
    "            \n",
    "    return theta\n",
    "\n",
    "theta = mini_batch_gradient_descent(50 , 64)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3302a30d-6577-41f0-a205-6c052a6e8bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447589562140646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Functions taken from in-class-exercise 10.28.24\n",
    "'''\n",
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias) #this is the dot product and take the sign. \n",
    "    return label\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2) #computing L2 norm \n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "#theta = [-.1 for _ in range(104)]\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561b491-cf82-4f68-9a9d-2b741166b676",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MINI-BATCH GRADIENT DESCENT WITH (EPSILON)- DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "315243fc-7e6d-4df3-bfb0-475b94ed338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy with epsilon = 1.0, epochs = 10, batch size = 64: 0.7588456435205662\n",
      "Final accuracy with epsilon = 0.5, epochs = 10, batch size = 64:  0.7799646174259177\n",
      "Final accuracy with epsilon = 1.0, epochs = 20, batch size = 55:  0.7904688191065901\n",
      "Final accuracy with epsilon = 0.5, epochs = 20, batch size = 55:  0.7602830605926582\n",
      "Final accuracy with epsilon = 1.0, epochs = 20, batch size = 70:  0.7829500221141088\n",
      "Final accuracy with epsilon = 1.0, epochs = 20, batch size = 75:  0.754312251216276\n"
     ]
    }
   ],
   "source": [
    "def epsilon_noisy_gradient_descent(epochs, epsilon, batch_size):\n",
    "    #Step 1: initalize all thetas \n",
    "    theta = [0 for _ in range(X_train.shape[1])]\n",
    "\n",
    "    #Step 2: splitting the epsilon and choosing sensitivity\n",
    "    epsilon_i = epsilon/epochs\n",
    "    sensitivity = 1 #?\n",
    "    \n",
    "    #Step 3: split data into mini_batches\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(all_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = laplace_mech_vec(grad_sum,sensitivity=sensitivity,epsilon=epsilon_i)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) \n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = epsilon_noisy_gradient_descent(10, 1.0, 64) #a smaller epsilon, accuracy is not as good. Noise can make the model worse. \n",
    "                                                    # If we increase iterations, it will make up for it. \n",
    "\n",
    "\n",
    "theta1 = epsilon_noisy_gradient_descent(10, 0.5, 64)\n",
    "theta2 = epsilon_noisy_gradient_descent(10, 1.0, 55)\n",
    "theta3 = epsilon_noisy_gradient_descent(10, 0.5, 55)\n",
    "theta4 = epsilon_noisy_gradient_descent(10, 1.0, 70)\n",
    "theta5 = epsilon_noisy_gradient_descent(10, 0.5, 70)\n",
    "\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 10, batch size = 64:', accuracy(theta))\n",
    "print('Final accuracy with epsilon = 0.5, epochs = 10, batch size = 64: ', accuracy(theta1))\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 20, batch size = 55: ', accuracy(theta2))\n",
    "print('Final accuracy with epsilon = 0.5, epochs = 20, batch size = 55: ', accuracy(theta3))\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 20, batch size = 70: ', accuracy(theta4))\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 20, batch size = 75: ', accuracy(theta5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b459b24-e776-42ef-9276-cf2509ff3521",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MINI-BATCH GRADIENT DESCENT WITH (EPSILON,DELTA)- DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d078f1-7ca3-4790-a515-06b99ccb745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/mgj8y39x4xvdgz8vbdv_wkyw0000gn/T/ipykernel_20975/3425499030.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return - (yi*xi) / (1+np.exp(exponent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy with epsilon = 1.0, epochs = 10, batch size = 64: 0.7610570544007077\n",
      "Final accuracy with epsilon = 0.5, epochs = 10, batch size = 64:  0.7224679345422379\n",
      "Final accuracy with epsilon = 1.0, epochs = 20, batch size = 55:  0.7367315347191509\n",
      "Final accuracy with epsilon = 0.5, epochs = 20, batch size = 55:  0.7203670942061035\n",
      "Final accuracy with epsilon = 1.0, epochs = 20, batch size = 70:  0.7304290137107474\n",
      "Final accuracy with epsilon = 1.0, epochs = 20, batch size = 75:  0.6640866873065016\n",
      "Final accuracy: 0.7610570544007077\n"
     ]
    }
   ],
   "source": [
    "def epsilon_delta_noisy_gradient_descent(epochs, epsilon, delta, batch_size):\n",
    "    #Step 1: initalize all thetas \n",
    "    theta = [0 for _ in range(X_train.shape[1])]\n",
    "\n",
    "    #Step 2: splitting the epsilons and delta over the num of iterations/epochs.\n",
    "    epsilon_i = epsilon/epochs\n",
    "    delta_i = delta/epochs\n",
    "\n",
    "    #Step 3: split data into mini_batches\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 2. Call L2_clip on each gradient\n",
    "            b = 3\n",
    "            clipped_grads = [L2_clip(g, b) for g in all_grads]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(clipped_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = gaussian_mech_vec(grad_sum,sensitivity=b,epsilon=epsilon_i,delta=delta_i)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) \n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = epsilon_delta_noisy_gradient_descent(10, 1.0, 1e-5,64)\n",
    "theta1 = epsilon_delta_noisy_gradient_descent(10, 0.5, 1e-5, 64)\n",
    "theta2 = epsilon_delta_noisy_gradient_descent(10, 1.0, 1e-5, 55)\n",
    "theta3 = epsilon_delta_noisy_gradient_descent(10, 0.5, 1e-5, 55)\n",
    "theta4 = epsilon_delta_noisy_gradient_descent(10, 1.0, 1e-5, 70)\n",
    "theta5 = epsilon_delta_noisy_gradient_descent(10, 0.5, 1e-5, 70)\n",
    "\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 10, batch size = 64:', accuracy(theta))\n",
    "print('Final accuracy with epsilon = 0.5, epochs = 10, batch size = 64: ', accuracy(theta1))\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 20, batch size = 55: ', accuracy(theta2))\n",
    "print('Final accuracy with epsilon = 0.5, epochs = 20, batch size = 55: ', accuracy(theta3))\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 20, batch size = 70: ', accuracy(theta4))\n",
    "print('Final accuracy with epsilon = 1.0, epochs = 20, batch size = 75: ', accuracy(theta5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88c63520-e78b-465b-8692-78caf2399bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m             theta \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m-\u001b[39m noisy_grad\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m theta\n\u001b[0;32m---> 34\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[43mvectorized_epsilon_delta_noisy_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#a smaller epsilon, accuracy is not as good. Noise can make the model worse. \u001b[39;00m\n\u001b[1;32m     35\u001b[0m                                                 \u001b[38;5;66;03m# If we increase iterations, it will make up for it. \u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy(theta))\n",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36mvectorized_epsilon_delta_noisy_gradient_descent\u001b[0;34m(epochs, epsilon, delta, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m split_to_mini_batches(X_train, y_train, batch_size)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train_batch, y_train_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m---> 15\u001b[0m     all_grads \u001b[38;5;241m=\u001b[39m [gradient_vectorized(theta,X_train_batch[i],y_train_batch[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train_batch))]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# 2. Call L2_clip on each gradient\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m split_to_mini_batches(X_train, y_train, batch_size)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train_batch, y_train_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m---> 15\u001b[0m     all_grads \u001b[38;5;241m=\u001b[39m [\u001b[43mgradient_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train_batch))]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# 2. Call L2_clip on each gradient\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m, in \u001b[0;36mgradient_vectorized\u001b[0;34m(theta, x, y, lambda_param)\u001b[0m\n\u001b[1;32m     26\u001b[0m m, n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     28\u001b[0m exponent \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39mdot(theta))\n\u001b[0;32m---> 29\u001b[0m gradient_loss \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m-\u001b[39m (\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexponent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Reshape to handle case where x is csr_matrix\u001b[39;00m\n\u001b[1;32m     32\u001b[0m gradient_loss\u001b[38;5;241m.\u001b[39mreshape(theta\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "def vectorized_epsilon_delta_noisy_gradient_descent(epochs, epsilon, delta, batch_size):\n",
    "    #Step 1: initalize all thetas \n",
    "    theta = [0 for _ in range(X_train.shape[1])]\n",
    "\n",
    "    #Step 2: splitting the epsilons and delta over the num of iterations/epochs.\n",
    "    epsilon_i = epsilon/epochs\n",
    "    delta_i = delta/epochs\n",
    "\n",
    "    #Step 3: split data into mini_batches\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient_vectorized(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 2. Call L2_clip on each gradient\n",
    "            b = 3\n",
    "            clipped_grads = [L2_clip(g, b) for g in all_grads]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(clipped_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = gaussian_mech_vec(grad_sum,sensitivity=b,epsilon=epsilon_i,delta=delta_i)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) \n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = vectorized_epsilon_delta_noisy_gradient_descent(10, 1.0, 1e-5,64) #a smaller epsilon, accuracy is not as good. Noise can make the model worse. \n",
    "                                                # If we increase iterations, it will make up for it. \n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dc2eb-c47d-4a26-a025-d3f88167492a",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MINI-BATCH GRADIENT DESCENT WITH RÃ‰NYI DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88a07501-1e14-4a5f-a207-f4079bebcd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy with epsilon_bar = 0.1, epochs = 10, alpha = 20, batch size = 64: 0.770234409553295\n",
      "Final accuracy with epsilon_bar = 0.2, epochs = 10, alpha = 20, batch size = 64:  0.7767580716497126\n",
      "Final accuracy with epsilon_bar = 0.3, epochs = 10, alpha = 20, batch size = 55:  0.7705661211853162\n",
      "Final accuracy with epsilon_bar = 0.3, epochs = 10, alpha = 15, batch size = 55:  0.7655904467049978\n",
      "Final accuracy with epsilon_bar = 0.1, epochs = 10, alpha = 15, batch size = 70:  0.7535382574082264\n",
      "Final accuracy with epsilon_bar = 0.1, epochs = 10, alpha = 25, batch size = 70:  0.7806280406899602\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Original functions taken from homework assignment 9\n",
    "'''\n",
    "def mini_batch_noisy_gradient_descent_RDP(epochs, epsilon_bar, alpha, batch_size):\n",
    "    #Step 1: initalize all thetas \n",
    "    theta = [0 for _ in range(X_train.shape[1])]\n",
    "\n",
    "\n",
    "    #Step 3: split data into mini_batches\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 2. Call L2_clip on each gradient\n",
    "            b = 3\n",
    "            clipped_grads = [L2_clip(g, b) for g in all_grads]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(clipped_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = gaussian_mech_RDP_vec(grad_sum,sensitivity=b,alpha=alpha,epsilon=epsilon_bar)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) #MAYBE DO LEN(MINI_BATCH)\n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = mini_batch_noisy_gradient_descent_RDP(10, 0.1, 20, 64)\n",
    "theta1 = mini_batch_noisy_gradient_descent_RDP(10, 0.3, 20, 64)\n",
    "theta2 = mini_batch_noisy_gradient_descent_RDP(10, 0.3, 20, 55)\n",
    "theta3 = mini_batch_noisy_gradient_descent_RDP(10, 0.1, 15, 55)\n",
    "theta4 = mini_batch_noisy_gradient_descent_RDP(10, 0.1, 15, 70)\n",
    "theta5 = mini_batch_noisy_gradient_descent_RDP(10, 0.1, 25, 70)\n",
    "\n",
    "print('Final accuracy with epsilon_bar = 0.1, epochs = 10, alpha = 20, batch size = 64:', accuracy(theta))\n",
    "print('Final accuracy with epsilon_bar = 0.2, epochs = 10, alpha = 20, batch size = 64: ', accuracy(theta1))\n",
    "print('Final accuracy with epsilon_bar = 0.3, epochs = 10, alpha = 20, batch size = 55: ', accuracy(theta2))\n",
    "print('Final accuracy with epsilon_bar = 0.3, epochs = 10, alpha = 15, batch size = 55: ', accuracy(theta3))\n",
    "print('Final accuracy with epsilon_bar = 0.1, epochs = 10, alpha = 15, batch size = 70: ', accuracy(theta4))\n",
    "print('Final accuracy with epsilon_bar = 0.1, epochs = 10, alpha = 25, batch size = 70: ', accuracy(theta5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586097f-2b55-4200-b595-935bc51d9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Original functions taken from homework assignment 9\n",
    "'''\n",
    "def vectorized_mini_batch_noisy_gradient_descent_RDP(epochs, epsilon_bar, alpha, batch_size):\n",
    "    #Step 1: initalize all thetas \n",
    "    theta = [0 for _ in range(X_train.shape[1])]\n",
    "\n",
    "\n",
    "    #Step 3: split data into mini_batches\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient_vectorized(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 2. Call L2_clip on each gradient\n",
    "            b = 3\n",
    "            clipped_grads = [L2_clip(g, b) for g in all_grads]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(clipped_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = gaussian_mech_RDP_vec(grad_sum,sensitivity=b,alpha=alpha,epsilon=epsilon_bar)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) #MAYBE DO LEN(MINI_BATCH)\n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = vectorized_mini_batch_noisy_gradient_descent_RDP(10, 0.1, 20,64) \n",
    "\n",
    "                                                \n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071752f-64ed-412b-bd7d-387fdf43e02e",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MINI-BATCH GRADIENT DESCENT WITH zCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "413d1f27-e722-431c-9e88-26cade3cba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy with rho = 1.0, epochs = 10, batch size = 64: 0.7470145953118089\n",
      "Final accuracy with rho = 0.5, epochs = 10, batch size = 64:  0.7943387881468377\n",
      "Final accuracy with rho = 1.0, epochs = 20, batch size = 55:  0.804953560371517\n",
      "Final accuracy with rho = 0.5, epochs = 20, batch size = 55:  0.7919062361786819\n",
      "Final accuracy with rho = 1.0, epochs = 20, batch size = 70:  0.8108137992038921\n",
      "Final accuracy with rho = 1.0, epochs = 20, batch size = 75:  0.8147943387881469\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Original functions taken from homework assignment 9\n",
    "'''\n",
    "def mini_batch_noisy_gradient_descent_zCDP(epochs, rho, batch_size):\n",
    "    #IDEA: copy noisy_gradient_descent but use gaussian_mech_zCDP_vec to compute the noisy_gradient_sum\n",
    "    \n",
    "    #from the noisy_gradient_descent function provided above: \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 2. Call L2_clip on each gradient\n",
    "            b = 3\n",
    "            clipped_grads = [L2_clip(g, b) for g in all_grads]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(clipped_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = gaussian_mech_zCDP_vec(grad_sum,sensitivity=b,rho=rho)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) #MAYBE DO LEN(MINI_BATCH)\n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = mini_batch_noisy_gradient_descent_zCDP(10, 0.1, 64)\n",
    "\n",
    "theta1 = mini_batch_noisy_gradient_descent_zCDP(10, 0.2, 64)\n",
    "theta2 = mini_batch_noisy_gradient_descent_zCDP(10, 0.1, 55)\n",
    "theta3 = mini_batch_noisy_gradient_descent_zCDP(10, 0.2, 55)\n",
    "theta4 = mini_batch_noisy_gradient_descent_zCDP(10, 0.1, 70)\n",
    "theta5 = mini_batch_noisy_gradient_descent_zCDP(10, 0.2, 70)\n",
    "\n",
    "print('Final accuracy with rho = 1.0, epochs = 10, batch size = 64:', accuracy(theta))\n",
    "print('Final accuracy with rho = 0.5, epochs = 10, batch size = 64: ', accuracy(theta1))\n",
    "print('Final accuracy with rho = 1.0, epochs = 20, batch size = 55: ', accuracy(theta2))\n",
    "print('Final accuracy with rho = 0.5, epochs = 20, batch size = 55: ', accuracy(theta3))\n",
    "print('Final accuracy with rho = 1.0, epochs = 20, batch size = 70: ', accuracy(theta4))\n",
    "print('Final accuracy with rho = 1.0, epochs = 20, batch size = 75: ', accuracy(theta5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f61d294-7471-40e1-bbeb-61643d41a072",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m             theta \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m-\u001b[39m noisy_grad\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m theta\n\u001b[0;32m---> 33\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[43mvectorized_mini_batch_noisy_gradient_descent_zCDP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy(theta))\n",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36mvectorized_mini_batch_noisy_gradient_descent_zCDP\u001b[0;34m(epochs, rho, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m split_to_mini_batches(X_train, y_train, batch_size)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train_batch, y_train_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m---> 14\u001b[0m     all_grads \u001b[38;5;241m=\u001b[39m [gradient_vectorized(theta,X_train_batch[i],y_train_batch[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train_batch))]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 2. Call L2_clip on each gradient\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m split_to_mini_batches(X_train, y_train, batch_size)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train_batch, y_train_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m---> 14\u001b[0m     all_grads \u001b[38;5;241m=\u001b[39m [\u001b[43mgradient_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train_batch))]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 2. Call L2_clip on each gradient\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mgradient_vectorized\u001b[0;34m(theta, x, y, lambda_param)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03mGradient function for logistic regression without regularization.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mBased on the above logistic_regression\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m exponent \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39mdot(theta))\n\u001b[0;32m---> 26\u001b[0m gradient_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexponent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Reshape to handle case where x is csr_matrix\u001b[39;00m\n\u001b[1;32m     29\u001b[0m gradient_loss\u001b[38;5;241m.\u001b[39mreshape(theta\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Original functions taken from homework assignment 9\n",
    "'''\n",
    "def vectorized_mini_batch_noisy_gradient_descent_zCDP(epochs, rho, batch_size):\n",
    "    #IDEA: copy noisy_gradient_descent but use gaussian_mech_zCDP_vec to compute the noisy_gradient_sum\n",
    "    \n",
    "    #from the noisy_gradient_descent function provided above: \n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    for _ in range(epochs): #epochs = iterations\n",
    "        mini_batches = split_to_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "        for X_train_batch, y_train_batch in mini_batches:\n",
    "        \n",
    "            all_grads = [gradient_vectorized(theta,X_train_batch[i],y_train_batch[i]) for i in range(len(X_train_batch))]\n",
    "            \n",
    "            # 2. Call L2_clip on each gradient\n",
    "            b = 3\n",
    "            clipped_grads = [L2_clip(g, b) for g in all_grads]\n",
    "            \n",
    "            # 3. Take the sum of the clipped gradients and add noise\n",
    "            grad_sum = np.sum(clipped_grads, axis=0)\n",
    "    \n",
    "            #Sensitivity is correct, by clipping\n",
    "            noisy_grad_sum = gaussian_mech_zCDP_vec(grad_sum,sensitivity=b,rho=rho)\n",
    "    \n",
    "            noisy_grad = np.array(noisy_grad_sum )/ len(X_train_batch) #Danger: reveals the size of the training data (probably not a big deal but\n",
    "            # does violate DP) #MAYBE DO LEN(MINI_BATCH)\n",
    "            \n",
    "            theta = theta - noisy_grad\n",
    "    \n",
    "    return theta\n",
    "\n",
    "theta = vectorized_mini_batch_noisy_gradient_descent_zCDP(10, 0.1, 64)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
